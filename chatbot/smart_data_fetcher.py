"""
Smart data fetcher that retrieves only the required columns from CSV files.
Fetches data based on asset name, function, date, and selected columns.
"""

import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional, Union
import logging
from datetime import datetime

import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import (
    CHATBOT_ENTRY_DIR,
    CHATBOT_EXIT_DIR,
    CHATBOT_TARGET_DIR,
    CHATBOT_BREADTH_DIR,
    CHATBOT_ENTRY_CSV,
    CHATBOT_EXIT_CSV,
    CHATBOT_TARGET_CSV,
    CHATBOT_BREADTH_CSV,
    CSV_ENCODING,
    DATE_FORMAT
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SmartDataFetcher:
    """
    Fetches only the required columns from data files based on:
    - Signal type (entry/exit/portfolio_target_achieved/breadth)
    - Asset name (ticker)
    - Function name (trading strategy)
    - Date
    - Required columns

    Supports both folder-based (legacy) and consolidated CSV (new) data structures.
    """

    def __init__(self, use_consolidated_csvs=True):
        """Initialize the smart data fetcher."""
        self.use_consolidated_csvs = use_consolidated_csvs

        # Legacy folder-based paths
        self.entry_dir = Path(CHATBOT_ENTRY_DIR)
        self.exit_dir = Path(CHATBOT_EXIT_DIR)
        self.target_dir = Path(CHATBOT_TARGET_DIR)
        self.breadth_dir = Path(CHATBOT_BREADTH_DIR)

        # New consolidated CSV paths
        self.entry_csv = Path(CHATBOT_ENTRY_CSV)
        self.exit_csv = Path(CHATBOT_EXIT_CSV)
        self.target_csv = Path(CHATBOT_TARGET_CSV)
        self.breadth_csv = Path(CHATBOT_BREADTH_CSV)
    
    def fetch_data(
        self,
        signal_types: List[str],
        required_columns: Optional[List[str]],
        assets: Optional[List[str]] = None,
        functions: Optional[List[str]] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit_rows: Optional[int] = None
    ) -> Dict[str, pd.DataFrame]:
        """
        Fetch data from specified signal types with the required columns or ALL columns.

        Uses consolidated CSV files if available, falls back to folder-based approach.

        Args:
            signal_types: List of signal types to fetch from (entry, exit, portfolio_target_achieved, breadth)
            required_columns: List of column names to fetch, or None to fetch ALL columns (preserves CSV structure)
            assets: Optional list of asset/ticker names to filter by
            functions: Optional list of function names to filter by
            from_date: Optional start date (YYYY-MM-DD)
            to_date: Optional end date (YYYY-MM-DD)
            limit_rows: Optional limit on number of rows per signal type

        Returns:
            Dictionary mapping signal_type to DataFrame with fetched data
            {
                "entry": DataFrame with required/all columns,
                "exit": DataFrame with required/all columns,
                ...
            }
        """
        # Use consolidated CSVs if enabled and available, otherwise fall back to folder-based
        if self.use_consolidated_csvs:
            # Check if consolidated CSVs exist for requested signal types
            consolidated_available = all(
                self._get_consolidated_csv_path(signal_type).exists()
                for signal_type in signal_types
            )

            if consolidated_available:
                logger.info("üîÑ Using consolidated CSV files for data fetching")
                return self.fetch_data_consolidated(
                    signal_types=signal_types,
                    required_columns=required_columns,
                    assets=assets,
                    functions=functions,
                    from_date=from_date,
                    to_date=to_date,
                    limit_rows=limit_rows
                )

        # Fall back to folder-based approach
        logger.info("üîÑ Using folder-based data fetching (fallback)")
        result = {}

        for signal_type in signal_types:
            try:
                if signal_type == "breadth":
                    df = self._fetch_breadth_data(
                        required_columns=required_columns,
                        from_date=from_date,
                        to_date=to_date,
                        limit_rows=limit_rows
                    )
                else:
                    df = self._fetch_signal_type_data(
                        signal_type=signal_type,
                        required_columns=required_columns,
                        assets=assets,
                        functions=functions,
                        from_date=from_date,
                        to_date=to_date,
                        limit_rows=limit_rows
                    )

                if not df.empty:
                    result[signal_type] = df
                    logger.info(f"‚úÖ Fetched {len(df)} rows from {signal_type} with columns: {list(df.columns)}")
                    # Show sample data for debugging
                    if len(df) > 0:
                        logger.info(f"üìä Sample data preview from {signal_type}:")
                        for col in df.columns:
                            sample_val = df[col].iloc[0] if not df[col].empty else "N/A"
                            logger.info(f"   {col}: {sample_val}")
                else:
                    logger.warning(f"‚ùå No data fetched from {signal_type}")
                    logger.warning(f"   Requested columns: {required_columns}")
                    logger.warning(f"   Assets filter: {assets}")
                    logger.warning(f"   Functions filter: {functions}")
                    logger.warning(f"   Date range: {from_date} to {to_date}")

            except Exception as e:
                logger.error(f"Error fetching data from {signal_type}: {e}")

        return result
    
    def _fetch_signal_type_data(
        self,
        signal_type: str,
        required_columns: Optional[List[str]],
        assets: Optional[List[str]] = None,
        functions: Optional[List[str]] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit_rows: Optional[int] = None
    ) -> pd.DataFrame:
        """
        Fetch data from a signal type (entry/exit/portfolio_target_achieved).
        
        Args:
            signal_type: One of "entry", "exit", "portfolio_target_achieved"
            required_columns: List of column names to fetch, or None to fetch ALL columns
            assets: Optional list of assets to filter by
            functions: Optional list of functions to filter by
            from_date: Optional start date
            to_date: Optional end date
            limit_rows: Optional row limit
            
        Returns:
            DataFrame with fetched data
        """
        # Get base directory for signal type
        base_dir = self._get_signal_type_dir(signal_type)
        if not base_dir.exists():
            logger.warning(f"Directory does not exist: {base_dir}")
            return pd.DataFrame()
        
        all_data = []
        
        # Iterate through asset directories
        for asset_dir in base_dir.iterdir():
            if not asset_dir.is_dir():
                continue
            
            asset_name = asset_dir.name
            
            # Filter by assets if specified
            if assets and asset_name not in assets:
                continue
            
            # Iterate through function directories
            for function_dir in asset_dir.iterdir():
                if not function_dir.is_dir():
                    continue
                
                function_name = function_dir.name
                
                # Filter by functions if specified
                if functions and function_name not in functions:
                    continue
                
                # Get CSV files in date range
                csv_files = self._get_csv_files_in_range(
                    function_dir,
                    from_date,
                    to_date
                )
                
                # Read data from CSV files
                for csv_file in csv_files:
                    try:
                        df = self._read_csv_with_columns(csv_file, required_columns)
                        
                        if not df.empty:
                            # Add metadata columns
                            df['_signal_type'] = signal_type
                            df['_asset'] = asset_name
                            df['_function'] = function_name
                            df['_date'] = csv_file.stem  # filename is the date
                            
                            all_data.append(df)
                    
                    except Exception as e:
                        logger.error(f"Error reading {csv_file}: {e}")
        
        # Combine all data
        if not all_data:
            return pd.DataFrame()
        
        combined_df = pd.concat(all_data, ignore_index=True)
        
        # Apply row limit if specified
        if limit_rows and len(combined_df) > limit_rows:
            combined_df = combined_df.head(limit_rows)
        
        return combined_df

    def fetch_data_consolidated(
        self,
        signal_types: List[str],
        required_columns: Optional[List[str]],
        assets: Optional[List[str]] = None,
        functions: Optional[List[str]] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit_rows: Optional[int] = None
    ) -> Dict[str, pd.DataFrame]:
        """
        Fetch data from consolidated CSV files.

        Args:
            signal_types: List of signal types to fetch from (entry, exit, portfolio_target_achieved, breadth)
            required_columns: List of column names to fetch, or None to fetch ALL columns
            assets: Optional list of asset/ticker names to filter by
            functions: Optional list of function names to filter by
            from_date: Optional start date (YYYY-MM-DD)
            to_date: Optional end date (YYYY-MM-DD)
            limit_rows: Optional limit on number of rows per signal type

        Returns:
            Dictionary mapping signal_type to DataFrame with fetched data
        """
        result = {}

        for signal_type in signal_types:
            try:
                if signal_type == "breadth":
                    df = self._fetch_breadth_data_consolidated(
                        required_columns=required_columns,
                        from_date=from_date,
                        to_date=to_date,
                        limit_rows=limit_rows
                    )
                else:
                    df = self._fetch_signal_type_data_consolidated(
                        signal_type=signal_type,
                        required_columns=required_columns,
                        assets=assets,
                        functions=functions,
                        from_date=from_date,
                        to_date=to_date,
                        limit_rows=limit_rows
                    )

                if not df.empty:
                    result[signal_type] = df
                    logger.info(f"‚úÖ Fetched {len(df)} rows from {signal_type} (consolidated) with columns: {list(df.columns)}")

            except Exception as e:
                logger.error(f"Error fetching data from {signal_type} (consolidated): {e}")

        return result

    def _fetch_signal_type_data_consolidated(
        self,
        signal_type: str,
        required_columns: Optional[List[str]],
        assets: Optional[List[str]] = None,
        functions: Optional[List[str]] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit_rows: Optional[int] = None
    ) -> pd.DataFrame:
        """
        Fetch data from consolidated CSV for a signal type (entry/exit/target).

        Args:
            signal_type: One of "entry", "exit", "portfolio_target_achieved"
            required_columns: List of column names to fetch, or None to fetch ALL columns
            assets: Optional list of assets to filter by
            functions: Optional list of functions to filter by
            from_date: Optional start date
            to_date: Optional end date
            limit_rows: Optional row limit

        Returns:
            DataFrame with fetched data
        """
        # Get the consolidated CSV path for this signal type
        csv_path = self._get_consolidated_csv_path(signal_type)
        if not csv_path.exists():
            logger.warning(f"Consolidated CSV does not exist: {csv_path}")
            return pd.DataFrame()

        try:
            # Read the consolidated CSV
            df = pd.read_csv(csv_path, encoding=CSV_ENCODING)

            if df.empty:
                return pd.DataFrame()

            # Extract symbol from "Symbol, Signal, Signal Date/Price[$]" column
            symbol_col = 'Symbol, Signal, Signal Date/Price[$]'
            if symbol_col in df.columns and assets:
                # Extract symbol (first part before comma)
                df['_extracted_symbol'] = df[symbol_col].str.split(',').str[0].str.strip()
                df = df[df['_extracted_symbol'].isin(assets)]
                df = df.drop(columns=['_extracted_symbol'])

            # Filter by Function column
            if 'Function' in df.columns and functions:
                df = df[df['Function'].isin(functions)]

            # Extract and filter by signal date if needed
            if from_date or to_date:
                if symbol_col in df.columns:
                    # Extract date from "Symbol, Signal, Signal Date/Price[$]" column
                    # Format: "SYMBOL, Long/Short, YYYY-MM-DD (Price: X.XX)"
                    import re
                    def extract_date(text):
                        if pd.isna(text):
                            return None
                        match = re.search(r'(\d{4}-\d{2}-\d{2})', str(text))
                        return match.group(1) if match else None
                    
                    df['_extracted_date'] = df[symbol_col].apply(extract_date)
                    df['_extracted_date'] = pd.to_datetime(df['_extracted_date'], errors='coerce')

                    if from_date:
                        from_date_obj = pd.to_datetime(from_date)
                        df = df[df['_extracted_date'] >= from_date_obj]

                    if to_date:
                        to_date_obj = pd.to_datetime(to_date)
                        df = df[df['_extracted_date'] <= to_date_obj]
                    
                    df = df.drop(columns=['_extracted_date'])

            # Apply column selection
            if required_columns:
                available_columns = df.columns.tolist()
                columns_to_keep = [col for col in required_columns if col in available_columns]
                if columns_to_keep:
                    df = df[columns_to_keep]
                else:
                    logger.warning(f"None of the required columns found in {signal_type} consolidated CSV")
                    return pd.DataFrame()
            # If required_columns is None, keep all columns

            # Apply row limit
            if limit_rows and len(df) > limit_rows:
                df = df.head(limit_rows)

            return df

        except Exception as e:
            logger.error(f"Error reading consolidated CSV {csv_path}: {e}")
            return pd.DataFrame()

    def _fetch_breadth_data_consolidated(
        self,
        required_columns: Optional[List[str]],
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit_rows: Optional[int] = None
    ) -> pd.DataFrame:
        """
        Fetch breadth data from consolidated CSV.

        Args:
            required_columns: List of column names to fetch, or None to fetch ALL columns
            from_date: Optional start date
            to_date: Optional end date
            limit_rows: Optional row limit

        Returns:
            DataFrame with fetched data
        """
        if not self.breadth_csv.exists():
            logger.warning(f"Breadth consolidated CSV does not exist: {self.breadth_csv}")
            return pd.DataFrame()

        try:
            # Read the consolidated CSV
            df = pd.read_csv(self.breadth_csv, encoding=CSV_ENCODING)

            if df.empty:
                return pd.DataFrame()

            # Apply date filters - breadth CSV uses "Date" column (capitalized)
            if from_date or to_date:
                date_col = 'Date'
                if date_col in df.columns:
                    # Convert date to datetime for filtering
                    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')

                    if from_date:
                        from_date_obj = pd.to_datetime(from_date)
                        df = df[df[date_col] >= from_date_obj]

                    if to_date:
                        to_date_obj = pd.to_datetime(to_date)
                        df = df[df[date_col] <= to_date_obj]

            # Apply column selection
            if required_columns:
                available_columns = df.columns.tolist()
                columns_to_keep = [col for col in required_columns if col in available_columns]
                if columns_to_keep:
                    df = df[columns_to_keep]
                else:
                    logger.warning("None of the required columns found in breadth consolidated CSV")
                    return pd.DataFrame()
            # If required_columns is None, keep all columns

            # Apply row limit
            if limit_rows and len(df) > limit_rows:
                df = df.head(limit_rows)

            return df

        except Exception as e:
            logger.error(f"Error reading breadth consolidated CSV {self.breadth_csv}: {e}")
            return pd.DataFrame()

    def _get_consolidated_csv_path(self, signal_type: str) -> Path:
        """Get the consolidated CSV path for a signal type."""
        if signal_type == "entry":
            return self.entry_csv
        elif signal_type == "exit":
            return self.exit_csv
        elif signal_type == "portfolio_target_achieved":
            return self.target_csv
        elif signal_type == "breadth":
            return self.breadth_csv
        else:
            raise ValueError(f"Invalid signal type: {signal_type}")

    def _fetch_breadth_data(
        self,
        required_columns: Optional[List[str]],
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit_rows: Optional[int] = None
    ) -> pd.DataFrame:
        """
        Fetch breadth data (no assets/functions, just date-based files).
        
        Args:
            required_columns: List of column names to fetch, or None to fetch ALL columns
            from_date: Optional start date
            to_date: Optional end date
            limit_rows: Optional row limit
            
        Returns:
            DataFrame with fetched data
        """
        if not self.breadth_dir.exists():
            logger.warning(f"Breadth directory does not exist: {self.breadth_dir}")
            return pd.DataFrame()
        
        # Get CSV files in date range
        csv_files = self._get_csv_files_in_range(
            self.breadth_dir,
            from_date,
            to_date
        )
        
        all_data = []
        
        for csv_file in csv_files:
            try:
                df = self._read_csv_with_columns(csv_file, required_columns)
                
                if not df.empty:
                    # Add metadata
                    df['_signal_type'] = 'breadth'
                    df['_date'] = csv_file.stem
                    all_data.append(df)
            
            except Exception as e:
                logger.error(f"Error reading {csv_file}: {e}")
        
        # Combine all data
        if not all_data:
            return pd.DataFrame()
        
        combined_df = pd.concat(all_data, ignore_index=True)
        
        # Apply row limit if specified
        if limit_rows and len(combined_df) > limit_rows:
            combined_df = combined_df.head(limit_rows)
        
        return combined_df
    
    def _get_signal_type_dir(self, signal_type: str) -> Path:
        """Get the base directory for a signal type."""
        if signal_type == "entry":
            return self.entry_dir
        elif signal_type == "exit":
            return self.exit_dir
        elif signal_type == "portfolio_target_achieved":
            return self.target_dir
        elif signal_type == "breadth":
            return self.breadth_dir
        else:
            raise ValueError(f"Invalid signal type: {signal_type}")
    
    def _get_csv_files_in_range(
        self,
        directory: Path,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None
    ) -> List[Path]:
        """
        Get CSV files in a directory within the specified date range.
        
        Args:
            directory: Directory to search
            from_date: Start date (YYYY-MM-DD)
            to_date: End date (YYYY-MM-DD)
            
        Returns:
            List of CSV file paths
        """
        csv_files = sorted(list(directory.glob("*.csv")))
        
        if not from_date and not to_date:
            return csv_files
        
        # Filter by date range
        filtered_files = []
        
        for csv_file in csv_files:
            file_date_str = csv_file.stem
            
            # Try to parse the date from filename
            try:
                file_date = datetime.strptime(file_date_str, DATE_FORMAT)
                
                # Check if within range
                if from_date:
                    from_date_obj = datetime.strptime(from_date, DATE_FORMAT)
                    if file_date < from_date_obj:
                        continue
                
                if to_date:
                    to_date_obj = datetime.strptime(to_date, DATE_FORMAT)
                    if file_date > to_date_obj:
                        continue
                
                filtered_files.append(csv_file)
            
            except ValueError:
                # If filename is not a date, skip it
                logger.warning(f"Could not parse date from filename: {csv_file.name}")
                continue
        
        return filtered_files
    
    def _read_csv_with_columns(
        self,
        csv_file: Path,
        required_columns: Optional[List[str]]
    ) -> pd.DataFrame:
        """
        Read a CSV file and return the required columns or ALL columns if None specified.
        Uses flexible matching: exact match, partial match, or semantic match.
        
        Args:
            csv_file: Path to CSV file
            required_columns: List of column names to read, or None to fetch ALL columns
            
        Returns:
            DataFrame with the required columns or ALL columns (preserving original CSV structure)
        """
        try:
            # First read just the header to see what columns are available
            df_header = pd.read_csv(csv_file, nrows=0, encoding=CSV_ENCODING)
            available_columns = df_header.columns.tolist()
            
            logger.info(f"üìÅ Reading {csv_file.name}")
            logger.info(f"üìä Available columns ({len(available_columns)}): {available_columns}")
            
            # If no specific columns requested, return ALL columns to preserve original CSV structure
            if required_columns is None:
                logger.info(f"üéØ Fetching ALL columns to preserve original CSV structure")
                df = pd.read_csv(csv_file, encoding=CSV_ENCODING)
                logger.info(f"üìà Loaded {len(df)} rows with {len(df.columns)} columns from {csv_file.name}")
                return df
            
            # Find which required columns exist in this file using flexible matching
            logger.info(f"üéØ Required columns ({len(required_columns)}): {required_columns}")
            columns_to_read = self._match_columns_flexibly(required_columns, available_columns)
            
            logger.info(f"‚úÖ Matched columns ({len(columns_to_read)}): {columns_to_read}")
            
            if not columns_to_read:
                logger.warning(f"‚ùå None of the required columns found in {csv_file.name}")
                logger.warning(f"   Required: {required_columns}")
                logger.warning(f"   Available: {available_columns}")
                return pd.DataFrame()
            
            # Read only the required columns
            df = pd.read_csv(csv_file, usecols=columns_to_read, encoding=CSV_ENCODING)
            
            logger.info(f"üìà Loaded {len(df)} rows with {len(df.columns)} columns from {csv_file.name}")
            
            return df
        
        except Exception as e:
            logger.error(f"Error reading CSV {csv_file}: {e}")
            return pd.DataFrame()
    
    def _match_columns_flexibly(
        self,
        required_columns: List[str],
        available_columns: List[str]
    ) -> List[str]:
        """
        Match required columns to available columns using flexible matching.
        
        Matching strategies:
        1. Exact match (case-insensitive)
        2. Partial match (column contains required keyword)
        3. Semantic match (e.g., "target" matches "target_1", "target_price", etc.)
        
        Args:
            required_columns: List of column names requested
            available_columns: List of column names in the CSV
            
        Returns:
            List of actual column names to read from CSV
        """
        matched_columns = []
        
        for req_col in required_columns:
            req_col_lower = req_col.lower().strip()
            
            # Strategy 1: Exact match (case-insensitive)
            found_exact = False
            for avail_col in available_columns:
                if avail_col.lower().strip() == req_col_lower:
                    if avail_col not in matched_columns:
                        matched_columns.append(avail_col)
                    found_exact = True
                    break
            
            if not found_exact:
                # Strategy 2: Smart partial matching with preference for better matches
                best_match = None
                best_score = 0
                
                for avail_col in available_columns:
                    if avail_col in matched_columns:
                        continue  # Skip already matched columns
                        
                    avail_col_lower = avail_col.lower().strip()
                    
                    # Calculate match score
                    score = 0
                    
                    # High score for exact substring match
                    if req_col_lower in avail_col_lower:
                        score += 100
                    
                    # Medium score for containing key words
                    req_words = set(req_col_lower.replace('[', ' ').replace(']', ' ').replace('(', ' ').replace(')', ' ').replace(',', ' ').split())
                    avail_words = set(avail_col_lower.replace('[', ' ').replace(']', ' ').replace('(', ' ').replace(')', ' ').replace(',', ' ').split())
                    
                    common_words = req_words & avail_words
                    if common_words:
                        score += len(common_words) * 10
                    
                    # Bonus for semantic relationships
                    if self._are_semantically_related(req_col_lower, avail_col_lower):
                        score += 5
                    
                    # Update best match
                    if score > best_score and score > 10:  # Minimum threshold
                        best_score = score
                        best_match = avail_col
                
                if best_match:
                    matched_columns.append(best_match)
                    logger.info(f"‚úÖ Matched '{req_col}' to '{best_match}' (score: {best_score})")
                else:
                    logger.warning(f"‚ùå Could not match required column '{req_col}' to any available columns")
                    logger.warning(f"   Available columns: {available_columns}")
        
        logger.info(f"üîó Final column matching: {dict(zip(required_columns, matched_columns))}")
        return matched_columns
    
    def _are_semantically_related(self, col1: str, col2: str) -> bool:
        """
        Check if two column names are semantically related.
        
        Examples:
        - "target" matches "target_1", "target_price", "target_reached"
        - "entry" matches "entry_date", "entry_price", "entry_signal"
        - "performance" matches "current_performance", "performance_pct"
        
        Args:
            col1: First column name (lowercase)
            col2: Second column name (lowercase)
            
        Returns:
            True if semantically related
        """
        # Define keyword groups that are semantically related
        related_groups = [
            {'target', 'target_1', 'target_2', 'target_3', 'target_price', 'target_reached', 'target_hit'},
            {'entry', 'entry_date', 'entry_price', 'entry_signal', 'entry_time'},
            {'exit', 'exit_date', 'exit_price', 'exit_signal', 'exit_time'},
            {'performance', 'current_performance', 'performance_pct', 'perf', 'pnl'},
            {'price', 'current_price', 'close_price', 'open_price', 'close', 'open'},
            {'date', 'signal_date', 'entry_date', 'exit_date', 'timestamp'},
            {'signal', 'signal_type', 'signal_date', 'signal_strength'},
            {'volume', 'vol', 'avg_volume', 'volume_ratio'},
            {'rsi', 'rsi_14', 'rsi_value'},
            {'macd', 'macd_line', 'macd_signal', 'macd_hist'},
            {'bollinger', 'bb_upper', 'bb_lower', 'bb_mid', 'bb_width'},
            {'stochastic', 'stoch', 'stoch_k', 'stoch_d'},
            {'divergence', 'div', 'bullish_div', 'bearish_div'},
            {'trend', 'trendline', 'uptrend', 'downtrend'},
        ]
        
        # Check if both columns belong to the same semantic group
        for group in related_groups:
            # Check if any word from col1 or col2 is in this group
            col1_words = set(col1.replace('_', ' ').split())
            col2_words = set(col2.replace('_', ' ').split())
            
            if (any(word in group for word in col1_words) and 
                any(word in group for word in col2_words)):
                return True
        
        return False
    
    def get_data_summary(
        self,
        signal_type: str,
        asset: Optional[str] = None,
        function: Optional[str] = None
    ) -> Dict:
        """
        Get summary information about available data.
        
        Args:
            signal_type: One of "entry", "exit", "portfolio_target_achieved", "breadth"
            asset: Optional asset name
            function: Optional function name
            
        Returns:
            Dictionary with summary info (available dates, row counts, etc.)
        """
        base_dir = self._get_signal_type_dir(signal_type)
        
        if signal_type == "breadth":
            csv_files = list(base_dir.glob("*.csv"))
            return {
                "signal_type": signal_type,
                "num_files": len(csv_files),
                "dates": [f.stem for f in sorted(csv_files)]
            }
        
        # For entry/exit/portfolio_target_achieved
        if asset and function:
            function_dir = base_dir / asset / function
            if function_dir.exists():
                csv_files = list(function_dir.glob("*.csv"))
                return {
                    "signal_type": signal_type,
                    "asset": asset,
                    "function": function,
                    "num_files": len(csv_files),
                    "dates": [f.stem for f in sorted(csv_files)]
                }
        
        return {"signal_type": signal_type, "error": "Invalid parameters"}

    def add_data_to_consolidated_csv(
        self,
        signal_type: str,
        new_data: pd.DataFrame,
        deduplicate: bool = True
    ) -> bool:
        """
        Add new data to a consolidated CSV file with optional deduplication.
        Preserves 'Signal First Origination Date' for existing records.

        Args:
            signal_type: One of "entry", "exit", "portfolio_target_achieved", "breadth"
            new_data: DataFrame with new data to add
            deduplicate: Whether to deduplicate based on unique keys

        Returns:
            True if successful, False otherwise
        """
        csv_path = self._get_consolidated_csv_path(signal_type)

        try:
            # Read existing data if file exists
            if csv_path.exists():
                existing_data = pd.read_csv(csv_path, encoding=CSV_ENCODING)
            else:
                existing_data = pd.DataFrame()

            # Add metadata columns if needed
            if signal_type != "breadth":
                # For entry/exit/portfolio_target_achieved, add metadata if not present
                if 'symbol' not in new_data.columns and 'symbol' in existing_data.columns:
                    # Try to extract symbol from existing data patterns
                    pass  # Will be handled by calling code
            else:
                # For breadth, add date metadata if not present
                if 'date' not in new_data.columns and 'date' in existing_data.columns:
                    pass  # Will be handled by calling code

            # Add 'Signal First Origination Date' to new data if not present
            if signal_type != "breadth" and 'Signal First Origination Date' not in new_data.columns:
                # For new signals, use the signal_date as the first origination date
                if 'signal_date' in new_data.columns:
                    new_data['Signal First Origination Date'] = new_data['signal_date']
                else:
                    # Fallback to current date
                    from datetime import datetime
                    new_data['Signal First Origination Date'] = datetime.now().strftime('%Y-%m-%d')

            # Combine existing and new data
            combined_data = pd.concat([existing_data, new_data], ignore_index=True)

            if deduplicate and not combined_data.empty:
                combined_data = self._deduplicate_data_preserve_origination(combined_data, signal_type)

            # Remove metadata columns before saving (they're only used for deduplication)
            # But KEEP 'Signal First Origination Date' as it's a permanent column
            if signal_type == "breadth":
                # For breadth data, remove date and signal_type_meta
                metadata_columns = ['date', 'signal_type_meta']
            else:
                # For other signal types, remove standard metadata columns but NOT Signal First Origination Date
                metadata_columns = ['symbol', 'function', 'signal_date', 'signal_type', 'interval', 'asset_name', 'signal_type_meta']

            columns_to_drop = [col for col in metadata_columns if col in combined_data.columns]
            if columns_to_drop:
                combined_data = combined_data.drop(columns=columns_to_drop)

            # Save back to CSV
            combined_data.to_csv(csv_path, index=False, encoding=CSV_ENCODING)
            logger.info(f"‚úÖ Added {len(new_data)} rows to {signal_type} consolidated CSV (total: {len(combined_data)} rows)")

            return True

        except Exception as e:
            logger.error(f"Error adding data to {signal_type} consolidated CSV: {e}")
            return False

    def _deduplicate_data_preserve_origination(self, data: pd.DataFrame, signal_type: str) -> pd.DataFrame:
        """
        Deduplicate data based on unique keys for each signal type.
        Preserves 'Signal First Origination Date' from the earliest occurrence.

        Args:
            data: DataFrame to deduplicate
            signal_type: Signal type to determine deduplication key

        Returns:
            Deduplicated DataFrame with preserved origination dates
        """
        if data.empty:
            return data

        # Create unique keys
        if signal_type == "breadth":
            # For breadth: date + function
            if 'date' in data.columns and 'Function' in data.columns:
                data['unique_key'] = data.apply(
                    lambda row: f"{row['date']}|{row.get('Function', 'Unknown')}",
                    axis=1
                )
            else:
                logger.warning("Cannot deduplicate breadth data: missing date or Function columns")
                return data
        else:
            # For entry/exit/portfolio_target_achieved: symbol + signal_type + asset_name + function + interval + Signal Open Price
            required_cols = ['symbol', 'signal_type', 'asset_name', 'function', 'interval', 'Signal Open Price']
            if all(col in data.columns for col in required_cols):
                data['unique_key'] = data.apply(
                    lambda row: f"{row['symbol']}|{row['signal_type']}|{row['asset_name']}|{row['function']}|{row['interval']}|{row['Signal Open Price']}",
                    axis=1
                )
            else:
                logger.warning(f"Cannot deduplicate {signal_type} data: missing required columns {required_cols}")
                return data

        # For each unique key, preserve the earliest 'Signal First Origination Date'
        if 'Signal First Origination Date' in data.columns:
            # Group by unique_key and get the earliest origination date
            earliest_origination = data.groupby('unique_key')['Signal First Origination Date'].first().to_dict()

            # Remove duplicates, keeping the last (most recent) occurrence
            original_count = len(data)
            data = data.drop_duplicates(subset=['unique_key'], keep='last')

            # Restore the earliest origination date for each unique key
            data['Signal First Origination Date'] = data['unique_key'].map(earliest_origination)

            removed_count = original_count - len(data)
            if removed_count > 0:
                logger.info(f"üóëÔ∏è Removed {removed_count} duplicate rows from {signal_type} data (preserved origination dates)")
        else:
            # No origination date column, just deduplicate normally
            original_count = len(data)
            data = data.drop_duplicates(subset=['unique_key'], keep='last')
            removed_count = original_count - len(data)
            if removed_count > 0:
                logger.info(f"üóëÔ∏è Removed {removed_count} duplicate rows from {signal_type} data")

        data = data.drop(columns=['unique_key'])

        return data

    def _deduplicate_data(self, data: pd.DataFrame, signal_type: str) -> pd.DataFrame:
        """
        Deduplicate data based on unique keys for each signal type.

        Args:
            data: DataFrame to deduplicate
            signal_type: Signal type to determine deduplication key

        Returns:
            Deduplicated DataFrame
        """
        if data.empty:
            return data

        if signal_type == "breadth":
            # For breadth: date + function
            if 'date' in data.columns and 'Function' in data.columns:
                data['unique_key'] = data.apply(
                    lambda row: f"{row['date']}|{row.get('Function', 'Unknown')}",
                    axis=1
                )
            else:
                logger.warning("Cannot deduplicate breadth data: missing date or Function columns")
                return data
        else:
            # For entry/exit/portfolio_target_achieved: symbol + signal_type + asset_name + function + interval + Signal Open Price
            required_cols = ['symbol', 'signal_type', 'asset_name', 'function', 'interval', 'Signal Open Price']
            if all(col in data.columns for col in required_cols):
                data['unique_key'] = data.apply(
                    lambda row: f"{row['symbol']}|{row['signal_type']}|{row['asset_name']}|{row['function']}|{row['interval']}|{row['Signal Open Price']}",
                    axis=1
                )
            else:
                logger.warning(f"Cannot deduplicate {signal_type} data: missing required columns {required_cols}")
                return data

        # Remove duplicates, keeping the last (most recent) occurrence
        original_count = len(data)
        data = data.drop_duplicates(subset=['unique_key'], keep='last')
        data = data.drop(columns=['unique_key'])

        removed_count = original_count - len(data)
        if removed_count > 0:
            logger.info(f"üóëÔ∏è Removed {removed_count} duplicate rows from {signal_type} data")

        return data


if __name__ == "__main__":
    # Test the data fetcher
    fetcher = SmartDataFetcher()
    
    # Test 1: Fetch entry data for TSM
    print("\n" + "="*60)
    print("TEST 1: Fetch entry data for TSM")
    print("="*60)
    
    result = fetcher.fetch_data(
        signal_types=["entry"],
        required_columns=["Symbol", "Signal", "Current Mark to Market and Holding Period"],
        assets=["TSM"],
        from_date="2025-10-14",
        to_date="2025-10-14"
    )
    
    if "entry" in result:
        print(f"\nFetched {len(result['entry'])} rows")
        print(result['entry'].head())
    
    # Test 2: Fetch breadth data
    print("\n" + "="*60)
    print("TEST 2: Fetch breadth data")
    print("="*60)
    
    result = fetcher.fetch_data(
        signal_types=["breadth"],
        required_columns=["Function", "Bullish Asset vs Total Asset (%)", "Date"],
        from_date="2025-10-14"
    )
    
    if "breadth" in result:
        print(f"\nFetched {len(result['breadth'])} rows")
        print(result['breadth'].head())
