================================================================================
MINDWEALTH CHATBOT - IMPLEMENTATION COMPLETE (SIMPLIFIED)
================================================================================

Date: October 12, 2025
Status: ✅ FULLY IMPLEMENTED - SIMPLIFIED INTERFACE

================================================================================
OVERVIEW
================================================================================

A complete AI-powered chatbot system for analyzing trading signals and target
data using GPT-4o. The system automatically loads BOTH signal and target data,
providing a unified analysis interface.

Key Features:
  ✅ Automatic loading of BOTH signal and target data
  ✅ No need for signal_type parameter - always includes both!
  ✅ Multiple asset tracking
  ✅ Function-based filtering
  ✅ Date range filtering
  ✅ Conversation history/context management
  ✅ Smart data reuse optimization
  ✅ Automatic function extraction from user queries

================================================================================
DATA STRUCTURE
================================================================================

The system uses a dual-folder structure but loads from BOTH automatically:

chatbot/data/
├── signal/{asset}/{function}/YYYY-MM-DD.csv  # Trading signals
└── target/{asset}/{function}/YYYY-MM-DD.csv  # Target achievements
    └── all_targets.csv  # Master file for deduplication

**Key Points:**
- Target files use CURRENT DATE as filename (when added)
- Loaded data includes 'DataType' column ('signal' or 'target')
- All queries automatically fetch from BOTH folders

================================================================================
KEY FEATURES
================================================================================

1. UNIFIED DATA LOADING
   - Always loads from BOTH signal and target folders
   - Single query interface - no signal_type parameter needed
   - Data includes 'DataType' column to distinguish sources

2. DUPLICATE HANDLING
   - Signals: Deduplicated during conversion based on DEDUP_COLUMNS
   - Targets: Checked against master CSV before storing
   - Rejected duplicates tracked in conversion summary

3. FUNCTION EXTRACTION (AUTO)
   - Uses GPT-4o-mini to extract function names from queries
   - Falls back to loading ALL functions if none extracted
   - Can be disabled with auto_extract_functions=False

4. SMART CONTEXT REUSE
   - Caches data when same parameters used
   - Checks: tickers, dates, functions
   - Saves tokens and improves response time

5. CONVERSATION HISTORY
   - Persistent across queries in same session
   - Maintains context for follow-up questions
   - Session-based storage in chatbot/history/

================================================================================
CORE MODULES
================================================================================

1. chatbot_engine.py
   - Main interface for queries
   - Parameters: tickers, from_date, to_date, functions
   - Automatically loads from BOTH signal and target folders
   - Auto function extraction
   - Context management

2. data_processor.py
   - Loads CSV data from signal/ AND target/ folders simultaneously
   - Handles date filtering
   - Function filtering
   - Adds 'DataType' column to distinguish source
   - Data formatting for GPT-4o

3. history_manager.py
   - Manages conversation history
   - Session persistence
   - Metadata tracking

4. function_extractor.py
   - Extracts function names from user queries
   - Uses GPT-4o-mini for efficiency
   - Validates against available functions

5. config.py
   - Environment configuration
   - API keys
   - Directory paths
   - Deduplication settings

6. convert_signals_to_data_structure.py
   - Converts CSV files to data structure
   - Handles both signal and target types
   - Duplicate detection for targets
   - Master CSV management

================================================================================
CONFIGURATION (.env)
================================================================================

Required settings:
  OPENAI_API_KEY=your_api_key_here
  OPENAI_MODEL=gpt-4o
  MAX_TOKENS=4096
  TEMPERATURE=0.7
  MAX_ROWS_TO_INCLUDE=1000
  MAX_HISTORY_LENGTH=10
  DEDUP_COLUMNS=Date,Symbol,Close

================================================================================
USAGE EXAMPLES
================================================================================

1. QUERY DATA (automatically includes signal AND target):
   response, metadata = chatbot.query(
       user_message="What signals and targets exist for AAPL?",
       tickers=["AAPL"],
       from_date="2025-10-01",
       to_date="2025-10-10",
       functions=["TRENDPULSE"]  # Or None for all
   )
   
   # Data automatically includes BOTH signal and target data!
   # No signal_type parameter needed!

2. AUTO FUNCTION EXTRACTION:
   response, metadata = chatbot.query(
       user_message="Show me FRACTAL TRACK for AAPL",
       tickers=["AAPL"],
       functions=None,  # Will auto-extract "FRACTAL TRACK"
       auto_extract_functions=True  # Default
   )

3. CONTEXT REUSE:
   # First query loads data (from both signal AND target)
   response1, _ = chatbot.query(
       user_message="What signals?",
       tickers=["AAPL"],
       functions=["TRENDPULSE"]
   )
   
   # Second query reuses data (same parameters)
   response2, metadata = chatbot.query(
       user_message="What was the price?",
       tickers=["AAPL"],
       functions=["TRENDPULSE"]
   )
   # metadata['data_reused_from_history'] == True

================================================================================
DATA CONVERSION
================================================================================

Convert signals and targets to data structure:
  python chatbot/convert_signals_to_data_structure.py

This script:
  1. Converts outstanding_signal.csv → signal folder
  2. Converts target_signal.csv → target folder
  3. Creates all_targets.csv master file
  4. Checks for duplicates
  5. Reports conversion statistics

Target files use CURRENT DATE as the filename (date when target was added).

================================================================================
TESTING
================================================================================

Available test files:
  - test_signal_query.py: Tests data queries (signal+target)
  - test_history_context.py: Tests conversation history
  - test_function_extraction.py: Tests auto function extraction
  - demo.py: Complete demonstration

Run tests:
  source venv/bin/activate
  python chatbot/test_signal_query.py
  python chatbot/test_history_context.py
  python chatbot/test_function_extraction.py
  python chatbot/demo.py

================================================================================
FILE STRUCTURE
================================================================================

chatbot/
├── __init__.py
├── chatbot_engine.py          # Main chatbot interface
├── data_processor.py           # Data loading (both signal+target)
├── history_manager.py          # Conversation history
├── function_extractor.py       # Auto function extraction
├── config.py                   # Configuration
├── convert_signals_to_data_structure.py  # Data converter
├── demo.py                     # Demonstration
├── test_signal_query.py        # Query tests
├── test_history_context.py     # History tests
├── test_function_extraction.py # Extraction tests
├── data/
│   ├── signal/                 # Signal data
│   │   └── {asset}/
│   │       └── {function}/
│   │           └── YYYY-MM-DD.csv
│   └── target/                 # Target data
│       ├── all_targets.csv     # Master target file
│       └── {asset}/
│           └── {function}/
│               └── YYYY-MM-DD.csv (current date)
└── history/
    └── {session_id}.json

================================================================================
API REFERENCE
================================================================================

ChatbotEngine.query(
    user_message: str,
    tickers: Optional[List[str]] = None,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    functions: Optional[List[str]] = None,
    additional_context: Optional[str] = None,
    dedup_columns: Optional[List[str]] = None,
    auto_extract_functions: bool = True
) -> Tuple[str, Dict]

**Note: Automatically loads data from BOTH signal and target folders!**

Returns:
  - response: AI-generated response text
  - metadata: {
      "tickers": [...],
      "from_date": "...",
      "to_date": "...",
      "functions": [...],
      "functions_auto_extracted": [...],
      "data_loaded": {...},  # Includes records from both signal & target
      "data_reused_from_history": bool,
      "tokens_used": {...}
    }

Helper Methods:
  - get_available_tickers() -> List[str]  # From both signal & target
  - get_available_functions(ticker=None) -> List[str]  # From both
  - get_session_id() -> str
  - get_conversation_history() -> List[Dict]
  - clear_history() -> None

================================================================================
PERFORMANCE
================================================================================

Optimizations:
  ✓ Smart data caching (reuse when parameters match)
  ✓ Efficient CSV loading with pandas
  ✓ Deduplication during conversion
  ✓ Target duplicate checking before storage
  ✓ Minimal token usage with function extraction (GPT-4o-mini)
  ✓ History-based context management
  ✓ Combined signal+target loading in single pass

Token Usage:
  - Function extraction: ~50-100 tokens (GPT-4o-mini)
  - Query response: ~1000-5000 tokens (GPT-4o)
  - Context reuse: Saves 500-2000 tokens per query

Data Loading:
  - Signal CSVs: 213 files
  - Target CSVs: 24 files (23 data + 1 master)
  - Combined automatically on every query

================================================================================
UPDATES IN THIS VERSION
================================================================================

✅ SIMPLIFIED INTERFACE:
  - Removed signal_type parameter completely
  - Always loads from BOTH signal and target folders
  - Single unified query interface
  - DataType column added to distinguish source

✅ Other Improvements:
  - Target files use current date as filename
  - Master CSV for target deduplication
  - Updated all test files
  - Demo includes both signals and targets
  - All Manus-related code removed

================================================================================
KEY SIMPLIFICATION
================================================================================

**BEFORE (Complex):**
```python
# Had to specify signal_type
response = chatbot.query(
    user_message="What signals?",
    tickers=["AAPL"],
    signal_type="signal"  # Required parameter
)
```

**NOW (Simplified):**
```python
# Automatically includes BOTH signal and target!
response = chatbot.query(
    user_message="What signals and targets?",
    tickers=["AAPL"]
    # No signal_type needed - loads everything!
)

# Data includes 'DataType' column to distinguish:
# - 'signal' for trading signals
# - 'target' for target achievements
```

================================================================================
SUPPORT & DOCUMENTATION
================================================================================

For detailed examples and usage:
  - See demo.py for comprehensive demonstrations
  - Check test files for specific use cases
  - Review config.py for configuration options

All systems operational and tested ✅

================================================================================
END OF IMPLEMENTATION SUMMARY
================================================================================
